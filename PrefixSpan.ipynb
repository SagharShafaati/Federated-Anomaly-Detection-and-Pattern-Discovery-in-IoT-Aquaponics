{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, AutoDateLocator\n",
    "from dateutil import parser, tz\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"UnknownTimezoneWarning\")\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"your_path\"\n",
    "RESULT_PATH = \"your_path\"\n",
    "\n",
    "# Filenames\n",
    "file_names = [f\"IoTpond{i}.csv\" for i in range(1, 13) if i != 5]  # Exclude Pond 5\n",
    "\n",
    "# Define timezone mapping for CET\n",
    "tzinfos = {\"CET\": tz.gettz(\"CET\")}\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(file_names, data_path):\n",
    "    datasets = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Validate necessary columns\n",
    "        required_columns = ['Date/Time', 'Temperature(C)', 'Turbidity(NTU)', 'Dissolved Oxygen(g/ml)',\n",
    "                            'PH', 'Ammonia(g/ml)', 'Nitrate(g/ml)', 'Population', 'Fish_Length(cm)', 'Fish_Weight(g)']\n",
    "        if not all(col in data.columns for col in required_columns):\n",
    "            print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "            continue\n",
    "\n",
    "        # Parse Date/Time and handle timezone\n",
    "        try:\n",
    "            data['Date/Time'] = data['Date/Time'].apply(\n",
    "                lambda x: parser.parse(x, tzinfos=tzinfos).replace(tzinfo=None) if pd.notna(x) else None\n",
    "            )\n",
    "            data.dropna(subset=['Date/Time'], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing datetime in {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert all required columns to numeric\n",
    "        for col in required_columns[1:]:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "        # Replace invalid values and handle missing data\n",
    "        data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        for col in required_columns[1:]:\n",
    "            if data[col].isna().sum() > 0:\n",
    "                data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "        # Scale features\n",
    "        try:\n",
    "            data[required_columns[1:]] = scaler.fit_transform(data[required_columns[1:]])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error scaling features in {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Ensure dataset has enough rows\n",
    "        if data.shape[0] < 5:  # Minimum 5 rows\n",
    "            print(f\"Skipping {file_name}: Insufficient data after cleaning ({data.shape[0]} rows).\")\n",
    "            continue\n",
    "\n",
    "        datasets.append(data)\n",
    "\n",
    "    if len(datasets) == 0:\n",
    "        print(\"No valid datasets found. Please check your data files.\")\n",
    "        return []\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = load_and_preprocess_data(file_names, DATA_PATH)\n",
    "\n",
    "if len(datasets) == 0:\n",
    "    raise ValueError(\"No valid datasets were loaded. Check your CSV files for errors.\")\n",
    "\n",
    "# Local anomaly detection using PrefixSpan\n",
    "def detect_prefixspan_patterns(data, min_support=0.1):\n",
    "    # Convert continuous features to discrete bins for rule mining\n",
    "    binned_data = data.copy()\n",
    "    for col in ['Temperature(C)', 'Turbidity(NTU)', 'Dissolved Oxygen(g/ml)',\n",
    "                'PH', 'Ammonia(g/ml)', 'Nitrate(g/ml)', 'Population']:\n",
    "        binned_data[col] = pd.qcut(binned_data[col], q=4, labels=False, duplicates='drop')\n",
    "\n",
    "    # Convert data into sequences\n",
    "    sequences = []\n",
    "    for _, row in binned_data.iterrows():\n",
    "        sequence = []\n",
    "        for col in binned_data.columns:\n",
    "            sequence.append(f\"{col}={row[col]}\")\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Perform PrefixSpan mining\n",
    "    ps = PrefixSpan(sequences)\n",
    "    ps.minlen = 2  # Minimum length of sequential patterns\n",
    "    ps.maxlen = 5  # Maximum length of sequential patterns\n",
    "    patterns = ps.frequent(min_support * len(sequences))\n",
    "\n",
    "    # Convert patterns to a DataFrame\n",
    "    patterns_df = pd.DataFrame(patterns, columns=[\"Support\", \"Pattern\"])\n",
    "    return patterns_df\n",
    "\n",
    "# Perform federated learning across ponds using parallelization\n",
    "def federated_learning_parallel(datasets, min_support=0.1):\n",
    "    global_patterns = pd.DataFrame()\n",
    "\n",
    "    def process_pond(data, pond_index):\n",
    "        patterns = detect_prefixspan_patterns(data, min_support=min_support)\n",
    "        patterns[\"Pond\"] = f\"Pond {pond_index + 1}\"\n",
    "        return patterns\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda x: process_pond(x[1], x[0]), enumerate(datasets)))\n",
    "\n",
    "    for patterns in results:\n",
    "        global_patterns = pd.concat([global_patterns, patterns], ignore_index=True)\n",
    "\n",
    "    return global_patterns\n",
    "\n",
    "# Save results to Excel\n",
    "def save_results_to_excel(patterns, datasets):\n",
    "    os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "    output_file = os.path.join(RESULT_PATH, \"FL_PrefixSpan_Patterns.xlsx\")\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        patterns.to_excel(writer, sheet_name=\"Global PrefixSpan Patterns\", index=False)\n",
    "\n",
    "        for i, data in enumerate(datasets):\n",
    "            data.to_excel(writer, sheet_name=f\"Pond {i + 1}\", index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Visualization\n",
    "def plot_all_visualizations(datasets, global_patterns):\n",
    "\n",
    "    # Time Series\n",
    "    fig, axs = plt.subplots(nrows=(len(datasets) + 4) // 5, ncols=5, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, data in enumerate(datasets):\n",
    "        axs[i].plot(data['Date/Time'], data['Fish_Length(cm)'], label=\"Fish Length\", linestyle='-', marker='o', markersize=3)\n",
    "        axs[i].plot(data['Date/Time'], data['Fish_Weight(g)'], label=\"Fish Weight\", linestyle='--', marker='x', markersize=3)\n",
    "        axs[i].set_title(f\"Pond {i + 1}\", fontsize=10)\n",
    "        axs[i].set_xlabel(\"Date/Time\", fontsize=8)\n",
    "        axs[i].set_ylabel(\"Growth Metrics\", fontsize=8)\n",
    "        axs[i].xaxis.set_major_locator(AutoDateLocator())\n",
    "        axs[i].xaxis.set_major_formatter(DateFormatter(\"%Y-%m-%d\"))\n",
    "        axs[i].tick_params(axis=\"x\", rotation=45, labelsize=7)\n",
    "        axs[i].tick_params(axis=\"y\", labelsize=7)\n",
    "        axs[i].legend(fontsize=7)\n",
    "\n",
    "    for j in range(len(datasets), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(RESULT_PATH, \"Ponds_Time_Series_Adjusted.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        print(f\"Time series visualization saved to {plot_path}\")\n",
    "\n",
    "\n",
    "    # Boxplots\n",
    "    num_datasets = len(datasets)\n",
    "    rows = (num_datasets + 4) // 5\n",
    "    fig, axs = plt.subplots(nrows=rows, ncols=5, figsize=(20, 6 * rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, data in enumerate(datasets):\n",
    "        data[['Temperature(C)', 'Turbidity(NTU)', 'PH']].boxplot(ax=axs[i], widths=0.7)\n",
    "        axs[i].set_title(f\"Pond {i + 1}\", fontsize=10)\n",
    "        axs[i].set_ylabel(\"Values\", fontsize=8)\n",
    "        axs[i].tick_params(axis=\"x\", labelsize=7)\n",
    "        axs[i].tick_params(axis=\"y\", labelsize=7)\n",
    "\n",
    "    for j in range(num_datasets, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(RESULT_PATH, \"Ponds_Boxplots_Adjusted_5cols.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Boxplots saved to {plot_path}\")\n",
    "\n",
    "    # Anomaly Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    global_patterns['Pond'].value_counts().plot(kind='bar')\n",
    "    plt.title(\"Anomaly Distribution Across Ponds\")\n",
    "    plt.xlabel(\"Ponds\")\n",
    "    plt.ylabel(\"Number of Anomalies\")\n",
    "    plot_path = os.path.join(RESULT_PATH, \"Anomaly_Distribution.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Anomaly distribution saved to {plot_path}\")\n",
    "\n",
    "# Evaluate relationship between anomalies and growth\n",
    "def evaluate_anomalies_and_growth(global_patterns, datasets):\n",
    "    results = []\n",
    "\n",
    "    for i, data in enumerate(datasets):\n",
    "        patterns = global_patterns[global_patterns[\"Pond\"] == f\"Pond {i + 1}\"]\n",
    "        correlation = data[[\"Fish_Length(cm)\", \"Fish_Weight(g)\"]].corr()\n",
    "        results.append({\"Pond\": f\"Pond {i + 1}\", \"Correlation\": correlation.iloc[0, 1], \"Patterns\": len(patterns)})\n",
    "\n",
    "    evaluation_df = pd.DataFrame(results)\n",
    "    evaluation_path = os.path.join(RESULT_PATH, \"Evaluation_PrefixSpan.xlsx\")\n",
    "    evaluation_df.to_excel(evaluation_path, index=False)\n",
    "    print(f\"Evaluation metrics saved to {evaluation_path}\")\n",
    "    return evaluation_df\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    global_patterns = federated_learning_parallel(datasets, min_support=0.1)\n",
    "    save_results_to_excel(global_patterns, datasets)\n",
    "    plot_all_visualizations(datasets, global_patterns)\n",
    "    evaluation_df = evaluate_anomalies_and_growth(global_patterns, datasets)\n",
    "    print(evaluation_df)\n",
    "\n",
    "# Ensure datasets are loaded\n",
    "if 'datasets' not in locals() or not datasets:\n",
    "    raise ValueError(\"Datasets not loaded. Ensure data preprocessing is complete before running the main script.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
